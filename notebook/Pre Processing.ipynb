{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objectives of pre-processing\n",
    "    \n",
    "- data should be 2D array\n",
    "- input data has to be numbers\n",
    "- no nan or inf\n",
    "- coloum are scaled to similar ranges (mean=0, variance=1)\n",
    "- coloums should not be coliner (cx1!=k*cx2)\n",
    "- rows should not be causally dependent\n",
    "- data should be 100 times larger then the number of coloums "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import rasterio\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define input datasets\n",
    "\n",
    "- features \n",
    "- targets \n",
    "- out of sample \n",
    "- area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 724 µs, total: 724 µs\n",
      "Wall time: 679 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "feature_file_paths = [\n",
    "Path(\"/g/data/ge3/sheece/LOC_distance_to_coast.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/mrvbf_9.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/relief_mrvbf_3s_mosaic.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/relief_elev_focalrange1000m_3s.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/relief_elev_focalrange300m_3s.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/saga_wetSM_85_resampled.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/tpi_300.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/slope_fill2.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/dem_fill.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/3dem_mag2.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/3dem_mag1_fin.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/3dem_mag0.fin.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/relief_roughness.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/LATITUDE_GRID1_clip.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Dose_2016.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Potassium_2016.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Thorium_2016.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Rad2016U_Th.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Rad2016K_Th.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/national_Wii_RF_multirandomforest_prediction.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/si_geol1.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/ceno_euc_aust1.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Grav_lane_clip.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-ND-NIR-GREEN.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-ND-RED-BLUE.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-ND-SWIR1-SWIR2.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg_BLUE+SWIR2.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-ND-SWIR1-NIR.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-CLAY-PC2.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-RED.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-GREEN.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-l8-all-85m-avg-BLUE.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-l8-all-85m-avg-NIR.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-SWIR1.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/be-30y-85m-avg-SWIR2.filled.lzw.nodata.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/s2-dpca-85m.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/water-85m.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/clim_EPA_albers.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/Clim_Prescott_LindaGregory.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/clim_PTA_albers.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/clim_WDA_albers.tif\"),\n",
    "Path(\"/g/data/ge3/sheece/clim_RSM_albers.tif\")\n",
    "]\n",
    "\n",
    "#target dataset small\n",
    "# /g/data/ge3/sheece/0_50cm_2021_albers_C_sm_T_resampled_small.shp\n",
    "#target dataset complete\n",
    "# /g/data/ge3/sheece/0_50cm_2021_albers_C_sm_T_resampled.shp\n",
    "target_file_path = Path(\"/g/data/ge3/sheece/0_50cm_2021_albers_C_sm_T_resampled.shp\")\n",
    "\n",
    "# define a shape for area of intrest\n",
    "area_of_interest_file_path = None\n",
    "\n",
    "#OOS\n",
    "# /g/data/ge3/sheece/0_50cm_2021_albers_C_oos.shp\n",
    "out_of_sample_file_path = Path(\"/g/data/ge3/sheece/0_50cm_2021_albers_C_oos.shp\")\n",
    "\n",
    "root = Path('/g/data/ge3/sheece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(root/\"old\"):\n",
    "    os.mkdir(root/\"old\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardising Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all datasets are in same projections \n",
    "import shutil\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "# check projection \n",
    "crs_epsg3577 = rasterio.crs.CRS.from_string('EPSG:3577')\n",
    "\n",
    "for feature_file_path in feature_file_paths:\n",
    "    with rasterio.open(feature_file_path) as src:\n",
    "        if crs_epsg3577 != src.crs:\n",
    "                print(\"Converting dataset: \"+str(feature_file_path))\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src.crs, \n",
    "                    crs_epsg3577, \n",
    "                    src.width, \n",
    "                    src.height, \n",
    "                    *src.bounds)\n",
    "                kwargs = src.meta.copy()\n",
    "                kwargs.update({'crs': crs_epsg3577,'transform': transform, 'width': width,'height': height})\n",
    "                \n",
    "                \n",
    "                with rasterio.open(root/ (feature_file_path.stem+'_resampled.tif'), 'w', **kwargs) as dst:\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, 1),\n",
    "                        destination=rasterio.band(dst, 1),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=crs_epsg3577,\n",
    "                        resampling=Resampling.nearest\n",
    "                    )\n",
    "                    print(dst.meta)\n",
    "                    \n",
    "                shutil.move(feature_file_path, root/(\"old/\"+feature_file_path.name))\n",
    "                output_path = root/ (feature_file_path.stem+'_resampled.tif')\n",
    "                print(str(output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply crop on target and feature datasets according to area of intrest \n",
    "import fiona\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "if area_of_interest_file_path is not None:\n",
    "    with fiona.open(area_of_interest_file_path) as shapefile:\n",
    "        geoms = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "    for feature_file_path in feature_file_paths:\n",
    "        if \"cropped\" not in feature_file_path.name:\n",
    "            print(\"Cropping : \"+feature_file_path.stem)\n",
    "            # load the raster, mask it by the polygon and crop it\n",
    "            with rasterio.open(feature_file_path) as src:\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "\n",
    "            # save the resulting raster  \n",
    "            out_meta.update({\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": out_image.shape[1],\n",
    "                    \"width\": out_image.shape[2],\n",
    "                    \"transform\": out_transform\n",
    "            })   \n",
    "\n",
    "            with rasterio.open(root/(feature_file_path.stem+'_cropped.tif'), \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "\n",
    "            shutil.move(feature_file_path, root/(\"old/\"+feature_file_path.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply crop vector files(.shp)\n",
    "import subprocess\n",
    "\n",
    "if area_of_interest_file_path is not None:\n",
    "    if \"cropped\" not in target_file_path.name:\n",
    "        print(\"Cropping: \"+target_file_path.name)\n",
    "        clipped_file = root/(target_file_path.stem+'_cropped.shp')\n",
    "        callstr = ['ogr2ogr',\n",
    "                   '-clipsrc',\n",
    "                   area_of_interest_file_path,\n",
    "                   clipped_file,\n",
    "                   target_file_path] \n",
    "        proc = subprocess.Popen(callstr, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "        stdout,stderr=proc.communicate()\n",
    "\n",
    "        shutil.move(target_file_path, root/(\"old/\"+target_file_path.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators ( C++ pointers) to inputs feature dataset and target dataset\n",
    "datasets = []\n",
    "input_feature_files = feature_file_paths\n",
    "for feature_file_path in feature_file_paths: \n",
    "    datasets.append(rasterio.open(feature_file_path))\n",
    "\n",
    "target_handle = fiona.open(target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crs': {'init': 'epsg:3577'},\n",
      " 'crs_wkt': 'PROJCS[\"GDA94 / Australian '\n",
      "            'Albers\",GEOGCS[\"GDA94\",DATUM[\"Geocentric_Datum_of_Australia_1994\",SPHEROID[\"GRS '\n",
      "            '1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6283\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4283\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"standard_parallel_1\",-18],PARAMETER[\"standard_parallel_2\",-36],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",132],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3577\"]]',\n",
      " 'driver': 'ESRI Shapefile',\n",
      " 'schema': {'geometry': 'Point',\n",
      "            'properties': OrderedDict([('con', 'float:24.15')])}}\n"
     ]
    }
   ],
   "source": [
    "pprint(target_handle.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 10982.55 MiB, increment: 10748.04 MiB\n",
      "CPU times: user 19min 46s, sys: 2min 35s, total: 22min 21s\n",
      "Wall time: 29min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "head_row = ['target']\n",
    "for input_feature_file in input_feature_files:\n",
    "    head_row.append(input_feature_file.stem)\n",
    "\n",
    "csv_rowlist = [head_row]\n",
    "with open(root/'input_dataset.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for target in target_handle:\n",
    "        value = target[\"properties\"][\"con\"]\n",
    "        x, y = target[\"geometry\"][\"coordinates\"]\n",
    "        new_row = [value]\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            new_row.append(next(dataset.sample([(x, y)]))[0])\n",
    "        csv_rowlist.append(new_row)\n",
    "\n",
    "        if len(csv_rowlist)%1000==0:\n",
    "            writer.writerows(csv_rowlist)\n",
    "            csv_rowlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_handle.close()\n",
    "for dataset in datasets: \n",
    "    dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 12196.82 MiB, increment: 2280.30 MiB\n",
      "CPU times: user 30 s, sys: 1.43 s, total: 31.4 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "train = np.genfromtxt(str(root)+'/input_dataset.csv', delimiter=',')\n",
    "train = train.astype(np.float32)\n",
    "train = train[~np.isnan(train).any(axis=1)]\n",
    "train = train[~(train == -9999.0).any(axis=1)]\n",
    "\n",
    "with open(str(root)+'/input_dataset.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    head_row = next(reader)\n",
    "    \n",
    "pd.DataFrame(train).to_csv(str(root)+'/input_dataset.csv',header=head_row, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(root)+'/input_dataset.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    head_row = next(reader)\n",
    "    \n",
    "dtrain = xgb.DMatrix(str(root)+'/input_dataset.csv?format=csv&label_column=0',nthread=-1,feature_names=head_row[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489318"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.num_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
